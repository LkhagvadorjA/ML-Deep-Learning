{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOZ8Hxt/hp1dShsD8thaOSI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"aMgZGsVncnIF","executionInfo":{"status":"ok","timestamp":1617707287122,"user_tz":-480,"elapsed":2479,"user":{"displayName":"Lhagvadorj Amarzaya","photoUrl":"","userId":"11576483647275485607"}}},"source":["import tensorflow as tf\n","import pandas as pd\n"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jRBOdZM8cNDt"},"source":["Prepare Dataset"]},{"cell_type":"code","metadata":{"id":"LBVrr_vUpp9F","executionInfo":{"status":"ok","timestamp":1617707287125,"user_tz":-480,"elapsed":2478,"user":{"displayName":"Lhagvadorj Amarzaya","photoUrl":"","userId":"11576483647275485607"}}},"source":["input = []\n","output = []"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"EuAKlEiXZUIo","executionInfo":{"status":"ok","timestamp":1617707287125,"user_tz":-480,"elapsed":2476,"user":{"displayName":"Lhagvadorj Amarzaya","photoUrl":"","userId":"11576483647275485607"}}},"source":["#read data from csv, tsv file\n","def read_file(file_name, columns, seperator, headerNum = 0, hasHeader=False):\n","  excel = pd.read_csv( file_name, seperator, header = headerNum ) if hasHeader else pd.read_csv( file_name, seperator, header=None )\n","  return pd.DataFrame(excel, columns = columns ) if hasHeader else pd.DataFrame(excel)"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pQGrM2cuO60g"},"source":["Process Data from words.tsv"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OWuK0UCGcLwF","executionInfo":{"status":"ok","timestamp":1617707287758,"user_tz":-480,"elapsed":3103,"user":{"displayName":"Lhagvadorj Amarzaya","photoUrl":"","userId":"11576483647275485607"}},"outputId":"5ba563fa-7ac5-4074-8ef9-a2343908f523"},"source":["# process data from file words.tv / result 2.tsv /\n","# input - list to store source word \n","# output - list to store derivation\n","def process_source_word(input, output):\n","  file_name = 'words.tsv'\n","  columns = [ 'word_id', 'lemma', 'POS', 'Category', 'Derivation', 'Source word', 'Derivation POS', 'Source POS', 'Inflection' ]\n","  words = read_file( file_name, columns, '\\t', 0, True)\n","  for index, row in words.iterrows():\n","    if row['Category'] == 'expert' or row['Category'] == 'expect' : continue \n","    if row['Category'] == 'yes': \n","      set_derivation(row, input, output)\n","    else:\n","      input.append( row['lemma'] )\n","      output.append( row['lemma'] )\n","\n","def set_derivation(row, input, output):\n","      derivation = row['Derivation'].replace('-', '')\n","      out_word = row['Source word'] + '+' + derivation\n","\n","      input.append( row['lemma'] )\n","      output.append(out_word)\n","\n","process_source_word(input, output)\n","print(len(input), len(output))\n","# for i in range( len(input) ) : print( input[i], output[i] )\n"],"execution_count":6,"outputs":[{"output_type":"stream","text":["4939 4939\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"U7BJ49axO_yY"},"source":["Process Data from mn-noun.tsv"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"touO0pdDPEzH","executionInfo":{"status":"ok","timestamp":1617707289998,"user_tz":-480,"elapsed":5339,"user":{"displayName":"Lhagvadorj Amarzaya","photoUrl":"","userId":"11576483647275485607"}},"outputId":"c5dd0924-bfc5-440a-f46b-404900d91b6e"},"source":["def process_noun_word(input, output):\n","  file_name = 'mn-noun.tsv'\n","  words = read_file( file_name, None, '\\t', 0 )\n","  \n","  for index, row in words.iterrows():\n","    input.append( row[1] )\n","    if not pd.isnull(row[2]):\n","      out_word = row[0] + '+' + row[2]\n","      output.append( out_word )\n","    else:\n","      output.append( row[1] )\n","\n","process_noun_word(input, output)\n","\n","print( len(input), len(output) )\n","# for i in range(10): \n","#   print( input[i], output[i] )"],"execution_count":7,"outputs":[{"output_type":"stream","text":["22577 22577\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CL4gWIKnWUQn"},"source":["Process Data from mn-verb-new.csv"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7kVDAlQAWXGj","executionInfo":{"status":"ok","timestamp":1617707292209,"user_tz":-480,"elapsed":7546,"user":{"displayName":"Lhagvadorj Amarzaya","photoUrl":"","userId":"11576483647275485607"}},"outputId":"67a4354e-f32a-4b89-c55a-7ede7ba99134"},"source":["def process_verb_word(input, output):\n","  file_name = 'mn-verb-new.csv'\n","  words = read_file( file_name, None, '\\t', 0 )\n","  \n","  for index, row in words.iterrows():\n","    if index == 0: continue\n","    input.append( row[1] )\n","    if not pd.isnull(row[2]):\n","      out_word = row[0] + '+' + row[3]\n","      output.append( out_word )\n","    else:\n","      output.append( row[1] )\n","\n","process_verb_word(input, output)\n","\n","print( len(input), len(output) )\n","# for i in range(10): \n","#   print( input[i], output[i] )"],"execution_count":8,"outputs":[{"output_type":"stream","text":["38128 38128\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5iqu_ozpYLFV"},"source":["Export to file"]},{"cell_type":"code","metadata":{"id":"3jErhHVpYNRa","executionInfo":{"status":"ok","timestamp":1617707292662,"user_tz":-480,"elapsed":7996,"user":{"displayName":"Lhagvadorj Amarzaya","photoUrl":"","userId":"11576483647275485607"}}},"source":["def export(input, output):\n","  list = []\n","  for i in range( len(input) ):\n","    list.append( { 'input': split(input[i]), 'output': split(output[i]) } )\n","  df = pd.DataFrame( list )\n","  df.to_csv('final.tsv', sep = '\\t', index=False, encoding='utf-8')\n","\n","def split(s):\n","  return s.replace(\"\", \" \")[1: -1]\n","\n","export(input, output)"],"execution_count":9,"outputs":[]}]}